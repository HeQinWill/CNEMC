{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241280f6-bd2f-46af-9cb3-3a966a5d0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import time\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8619191e-7327-4cc1-930a-aaab9efa9127",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = list(Path('Archive').rglob('*.csv'))\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8a83a9-05f1-48dd-857e-dd8ea1e11bff",
   "metadata": {},
   "source": [
    "### 检查文件数\n",
    "判断缺失的时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57650ba-1d54-4305-a75c-4feaaae2fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "systime = time.localtime(time.time())\n",
    "nowDate = time.strftime(\"%Y%m%d %H\", systime)\n",
    "\n",
    "should = pd.date_range('2022-1-1', nowDate, freq='H')\n",
    "exist = pd.to_datetime([f.stem for f in file_list])\n",
    "print(set(should) - set(exist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3468f5ba-b1f2-4dec-98c4-fc8081020782",
   "metadata": {},
   "source": [
    "### 合并数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d033e0f9-cc15-4ea4-9f8a-bae529055595",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 0\n",
    "if Path('cnemc.h5').exists():\n",
    "    ds_archive = xr.open_dataset('cnemc.h5')\n",
    "    START = len(ds_archive['timepoint'])\n",
    "    print(START)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efc4196-2c2c-43d2-adc4-455f1d47f066",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_list = []  # duplicated_list\n",
    "dfs = []\n",
    "\n",
    "for f in file_list[START:]:\n",
    "    df = pd.read_csv(f, parse_dates=['timepoint'])\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    if df.duplicated(subset='stationcode').sum() > 0:\n",
    "        d_list.append(f)\n",
    "    else:\n",
    "        dfs.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39db153-2233-4316-9b9a-3ef587f18ab2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 处理有重复的数据\n",
    "检查存在重复但其中一条数据某些字段存在缺失造成不一致的情况，此时手动删除缺失行再重新合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef05b8bf-fb8e-4585-93ea-226b86ed7d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in d_list[:]:\n",
    "    print(d)\n",
    "    df = pd.read_csv(d, parse_dates=['timepoint'])\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(df[df.duplicated(subset='stationcode', keep=False)].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc8ec35-5989-4f7e-aa4b-ed310221a932",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 将拼接后的表转为 xarray 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e5a8e6-0cd2-4f40-854a-6f0d001e7946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行拼接不要直接df.append，而要用这里的pd.concat(dfs)来提速\n",
    "df_all = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c723d0-f826-4e52-9260-139b9a3fecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_xarray(df):\n",
    "    df = df.set_index(['timepoint', 'stationcode'])\n",
    "    ds = df.to_xarray()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34443699-70ca-40c6-85f7-fc5ed55267e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = convert_xarray(df_all)\n",
    "\n",
    "for var in ['longitude', 'latitude', 'area', 'positionname']:\n",
    "    var_unique = []\n",
    "\n",
    "    for i in range(len(ds['stationcode'])):\n",
    "        # 将该站点所有观测转为列表\n",
    "        tmp = list(ds[var][:, i].values)\n",
    "        # 过滤掉列表中的nan\n",
    "        val = list(filter(lambda x: (type(x) == str) or (~np.isnan(x)), tmp))\n",
    "        # 当该站点没有冲突值的时候进行记录\n",
    "        if len(np.unique(val)) != 1:\n",
    "            print(tmp)\n",
    "        else:\n",
    "            var_unique.append(np.unique(val)[0])\n",
    "\n",
    "    # 替换掉原有数据的值\n",
    "    ds[var] = ds[var].isel(timepoint=0)\n",
    "    ds[var][:] = var_unique\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6610b0e-08a8-4b04-8d45-13bd70bf9adf",
   "metadata": {},
   "source": [
    "### 导出文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a39438-f810-4bf3-b090-0eb4f10e1513",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['co'] = ds['co']*1000\n",
    "ds['co_24h'] = ds['co_24h']*1000\n",
    "\n",
    "ds['latitude'].attrs = {'units': 'degree_north'}\n",
    "ds['longitude'].attrs = {'units': 'degree_east'}\n",
    "ds.attrs = {'units': 'All are µg/m3 except AQI which is unitless'}\n",
    "\n",
    "ds = ds.set_coords(['longitude', 'latitude',\n",
    "                    'area', 'positionname', 'primarypollutant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb9ff9c-9315-4fd3-b9ab-6a931d0aa6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选取需要保存的的变量\n",
    "selc_vars = list(ds.keys())[:]\n",
    "\n",
    "# 构造需要进行压缩的词典\n",
    "enco_vars = {}\n",
    "for d in selc_vars:\n",
    "    enco_vars[d] = {\"zlib\": True,\n",
    "                    \"complevel\": 9,\n",
    "                    \"dtype\": \"uint16\",\n",
    "                    '_FillValue': 65535\n",
    "                    }\n",
    "\n",
    "if START != 0:\n",
    "    ds = ds.merge(ds_archive)\n",
    "    Path('cnemc.h5').unlink()\n",
    "\n",
    "ds[selc_vars].to_netcdf('cnemc.h5',\n",
    "                        engine='netcdf4',\n",
    "                        encoding=enco_vars,\n",
    "                        mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897c4711-f9f0-4ccb-8203-6e2b4a341e7e",
   "metadata": {},
   "source": [
    "### 利用服务器上的历史数据替补\n",
    "需先检查一遍，可以合并后检查重复项或者看两个文件的 MD5 是否一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26382d7a-471f-4eb0-b1bc-1d26679041dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = Path('2022-02-24T00.csv')\n",
    "f2 = Path('2022-02-24T00(1).csv')\n",
    "h1 = hashlib.md5(open(f1, \"rb\").read()).hexdigest()\n",
    "h2 = hashlib.md5(open(f2, \"rb\").read()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15705af9-8480-43c0-95c3-1ff781a71a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if h1 == h2:\n",
    "    print('f1 == f2', h1)\n",
    "    f2.unlink()\n",
    "else:\n",
    "    t1 = f1.read_text()\n",
    "    t2 = '\\n'.join(f2.read_text().splitlines()[1:])\n",
    "    f1.unlink()\n",
    "    f2.unlink()\n",
    "    f1.write_text(t1+t2)\n",
    "\n",
    "    df = pd.read_csv(f1, parse_dates=['timepoint'])\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    if df.duplicated(subset='stationcode').sum() > 0:\n",
    "        print(df[df.duplicated(subset='stationcode', keep=False)].T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
